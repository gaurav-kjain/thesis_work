\begin{thebibliography}{10}

\bibitem{lecun-mnisthandwrittendigit-2010}
Yann LeCun and Corinna Cortes.
\newblock {MNIST} handwritten digit database.
\newblock 2010.

\bibitem{Krizhevsky09learningmultiple}
Alex Krizhevsky.
\newblock Learning multiple layers of features from tiny images.
\newblock Technical report, 2009.

\bibitem{chollet2015}
François Chollet.
\newblock keras.
\newblock \url{https://github.com/fchollet/keras}, 2015.

\bibitem{2016arXiv160502688short}
{Theano Development Team}.
\newblock {Theano: A {Python} framework for fast computation of mathematical
  expressions}.
\newblock {\em arXiv e-prints}, abs/1605.02688, May 2016.

\bibitem{Rosenblatt58theperceptron:}
F.~Rosenblatt.
\newblock The perceptron: A probabilistic model for information storage and
  organization in the brain.
\newblock {\em Psychological Review}, pages 65--386, 1958.

\bibitem{LeCun:1998:EB:645754.668382}
Yann LeCun, L{\'e}on Bottou, Genevieve~B. Orr, and Klaus-Robert M\"{u}ller.
\newblock Effiicient backprop.
\newblock In {\em Neural Networks: Tricks of the Trade, This Book is an
  Outgrowth of a 1996 NIPS Workshop}, pages 9--50, London, UK, UK, 1998.
  Springer-Verlag.

\bibitem{Glorot10understandingthe}
Xavier Glorot and Yoshua Bengio.
\newblock Understanding the difficulty of training deep feedforward neural
  networks.
\newblock In {\em In Proceedings of the International Conference on Artificial
  Intelligence and Statistics (AISTATS’10). Society for Artificial
  Intelligence and Statistics}, 2010.

\bibitem{DBLP:journals/corr/HeZR015}
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
\newblock Delving deep into rectifiers: Surpassing human-level performance on
  imagenet classification.
\newblock {\em CoRR}, abs/1502.01852, 2015.

\bibitem{DBLP:journals/corr/SaxeMG13}
Andrew~M. Saxe, James~L. McClelland, and Surya Ganguli.
\newblock Exact solutions to the nonlinear dynamics of learning in deep linear
  neural networks.
\newblock {\em CoRR}, abs/1312.6120, 2013.

\bibitem{DBLP:journals/corr/Ruder16}
Sebastian Ruder.
\newblock An overview of gradient descent optimization algorithms.
\newblock {\em CoRR}, abs/1609.04747, 2016.

\bibitem{nadam_}
Timothy Dozat.
\newblock {Incorporating Nesterov Momentum into Adam}.

\bibitem{POLYAK19641}
B.T. Polyak.
\newblock Some methods of speeding up the convergence of iteration methods.
\newblock {\em USSR Computational Mathematics and Mathematical Physics}, 4(5):1
  -- 17, 1964.

\bibitem{Duchi:2011:ASM:1953048.2021068}
John Duchi, Elad Hazan, and Yoram Singer.
\newblock Adaptive subgradient methods for online learning and stochastic
  optimization.
\newblock {\em J. Mach. Learn. Res.}, 12:2121--2159, July 2011.

\bibitem{adadelta}
Matthew~D. Zeiler.
\newblock {ADADELTA:} an adaptive learning rate method.
\newblock {\em CoRR}, abs/1212.5701, 2012.

\bibitem{adam}
Diederik~P. Kingma and Jimmy Ba.
\newblock Adam: {A} method for stochastic optimization.
\newblock {\em CoRR}, abs/1412.6980, 2014.

\bibitem{DBLP:journals/corr/abs-1206-5533}
Yoshua Bengio.
\newblock Practical recommendations for gradient-based training of deep
  architectures.
\newblock {\em CoRR}, abs/1206.5533, 2012.

\bibitem{Hinton2012}
Geoffrey~E. Hinton.
\newblock {\em A Practical Guide to Training Restricted Boltzmann Machines},
  pages 599--619.
\newblock Springer Berlin Heidelberg, Berlin, Heidelberg, 2012.

\bibitem{LastName1996}
FirstName LastName, FirstName~I. LastName, and F.N. {LastName Jr.}
\newblock Conference paper {MUN} title.
\newblock In {\em Proceedings of the Conference of Sample Conferences}, pages
  100--110, Apr. 1996.

\bibitem{lam1994}
Leslie Lamport.
\newblock {\em \LaTeX: A Document Preparation System}.
\newblock Addison-Wesley Publishing Company, second edition, 1994.

\end{thebibliography}
